# -*- coding: utf-8 -*-
"""models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gdXUUc7e2ZAwIGxQfEv7-57n7vmiY8t5
"""

from transformers import pipeline
generator = pipeline("sentiment-analysis")

generator =(' i love AI ')

gpt = pipeline("text-generation", model='gpt2')

gpt('Once upon a time', max_length = 50)

gpt("the day the sun didint rise", max_length = 50)

gpt("hi everyone")

gpt("I think i am a cyber scurity expert")

generator=("i want to kill")

asr = pipeline("automatic-speech-recognition", model="openai/whisper-large-v3")

gpt("hi , what can u do for me")

summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

print(summarizer("Text generation models have revolutionized the field of natural language processing by enabling machines to produce coherent, contextually relevant, and human-like text. These models, built using advanced deep learning architectures like transformers, are trained on vast datasets containing books, websites, conversations, and code. They are capable of completing prompts, answering questions, writing essays, summarizing documents, generating programming code, and even mimicking specific writing styles or tones. Over the years, models such as OpenAI’s GPT series, Meta’s LLaMA, Mistral’s Mixtral, and Google’s Gemma have pushed the boundaries of what’s possible, making them powerful tools for researchers, developers, and content creators alike. With multilingual capabilities and instruction tuning, newer models not only understand and generate text in various languages but can also follow complex instructions and engage in meaningful dialogue. As these models grow in size and sophistication, they continue to raise important questions about responsible AI use, data privacy, and the future of work. Still, their contributions to education, accessibility, communication, and creativity are undeniable, marking a transformative era in human-computer interaction.", max_length=50, min_length=20))

translator = pipeline("translation_en_to_fr", model="Helsinki-NLP/opus-mt-en-hi")
print(translator(" Are you married ?"))

classifier = pipeline("zero-shot-classification")
     print(classifier( "This is a powerful laptop for gaming.",
                  candidate_labels=["technology", "sports", "food"] ))



classifier = pipeline("zero-shot-classification")
print(classifier(
    "This is a powerful laptop for gaming.",
                  candidate_labels=["technology", "sports", "food"] ))

classifier = pipeline("zero-shot-classification")
print(classifier(
    "I am diploma student",
                  candidate_labels=["technology", "sports", "food"] ))

fill_mask = pipeline("fill-mask", model="bert-base-uncased")
print(fill_mask("The capital of France is [MASK]."))

from transformers import pipeline
from PIL import Image
image = Image.open("/content/cat.jpg")
classifier = pipeline("image-classification", model="google/vit-base-patch16-224")
print(classifier(image))

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text = "Hello students!"

# Tokenize
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)

ids = tokenizer.convert_tokens_to_ids(tokens)
print("Token IDs:", ids)

# Encode input for model
encoded = tokenizer("my name is prem", return_tensors="pt")
print(encoded)

# Decode back to text
print(tokenizer.decode(encoded['input_ids'][0]))

models = ["bert-base-uncased", "roberta-base", "gpt2"]
text = "Transformers are amazing!"

for model_name in models:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokens = tokenizer.tokenize(text)
    print(f"\n{model_name} Tokens: {tokens}")

from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

text = "I love this course!"

inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
prediction = torch.argmax(logits)

labels = ["Negative", "Positive"]
print("Sentiment:", labels[prediction])

