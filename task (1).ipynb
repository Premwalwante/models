{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKhxbQm0XaOo"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "sentiment = pipeline(\"sentiment-analysis\")\n",
        "print(sentiment(\"Ilove this course so much\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fpUo3p5YGnL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ze1sYyRlYeAA"
      },
      "outputs": [],
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "model=AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYaIS0dKY2VW"
      },
      "outputs": [],
      "source": [
        "inputs=tokenizer(\"I love this course!, but i want to kill my friend\",return_tensors=\"pt\")\n",
        "outputs=model(**inputs)\n",
        "logits=outputs.logits\n",
        "prediction=torch.argmax(logits)\n",
        "labels=[\"Negative\",\"Positive\"]\n",
        "print(\"Sentimet:\",labels[prediction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mORn6CUxZ7E2",
        "outputId": "9d891754-fe90-424a-f839-487636f58c46"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'generated_text': 'Today we are going to ptip but., we love your work.\\n\\nMy name is Noh-Lee. I am a young entrepreneur who wanted to build a startup but couldn\\'t. I have a passion for the best in tech and want to help people get started in the world of startups. I am here because I want to help everyone start their own company.\\n\\nIn this interview, you\\'re talking about the future of this world. What do you think the future holds for startups?\\n\\nI believe in the future of the internet. I think the internet will be the next big thing. I believe that the internet will give everyone a place to go, so that\\'s what we\\'re building.\\n\\nHow do you see the future of the internet?\\n\\nI think it\\'s going to be great. The internet is going to change the way you do business. We\\'re going to be able to say, \"This is a great company, you can do it.\"\\n\\nWhat are you working on now?\\n\\nWe are very excited about the future of the internet. It\\'s going to be a great moment for the internet. I\\'m really looking forward to seeing how it goes. I\\'m going to be going to conferences, working with people from around the world and I want to'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "generator=pipeline(\"text-generation\", model=\"gpt2\")\n",
        "print(generator(\"Today we are going to ptip but.,\",max_length=30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u8XqCFjalSc",
        "outputId": "d42fe829-d637-4338-ff11-ac23f9ebeab2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model=AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "inputs =tokenizer (\"Once upon a time\",return_tensors=\"pt\")\n",
        "outputs=model.generate(inputs[\"input_ids\"],max_length=30)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iRrArmscFhG",
        "outputId": "65fa38f4-523f-4ffe-edbe-e24d469966fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'summary_text': ' Artificial Intelligence (AI) refers to the ability of machines and computer systems to perform tasks that typically require human intelligence . AI is transforming industries'}]\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "text = \"Artificial Intelligence (AI) refers to the ability of machines and computer systems to perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, understanding natural language, and recognizing patterns in data. AI is transforming industries such as healthcare, finance, education, and transportation by enabling faster decision-making, automation of repetitive work, and the development of smart tools like chatbots, recommendation systems, and self-driving cars. With advances in deep learning and neural networks, AI systems are becoming more accurate and capable of adapting to complex environments. However, the rise of AI also raises ethical concerns, including job displacement, data privacy, and the responsible use of technology. As AI continues to evolve, it holds great promise to enhance human life while also demanding careful regulation and oversight.\"\n",
        "print(summarizer(text,max_length=30,min_length=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFYY_dByd2Xf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "print(translator(\"I like this course so much \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3m9JlRdfFJM",
        "outputId": "c3f9df20-9b19-4e16-d41d-04bfa1a5e670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'translation_text': '‡§Æ‡•à‡§Ç ‡§á‡§∏ ‡§ï‡•ã‡§∞‡•ç‡§∏ ‡§á‡§§‡§®‡§æ ‡§™‡§∏‡§Ç‡§¶ ‡§π‡•à'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "qa = pipeline (\"question-answering\")\n",
        "result=qa({\"question\":\"What is the capital of India?\",\"context\":\"The capital of India is New Delhi\"})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQjdJa7Kfc7z",
        "outputId": "e201ad35-e2f4-4343-802d-28a27193ea6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'score': 0.9967657327651978, 'start': 24, 'end': 33, 'answer': 'New Delhi'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize pipelines\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "summarizer_pipeline = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "translator_pipeline = pipeline(\"translation_en_to_hi\", model=\"Helsinki-NLP/opus-mt-en-hi\")\n",
        "\n",
        "def show_menu():\n",
        "    print(\"\\nü§ñ Welcome to AICLI Assistant\")\n",
        "    print(\"Please choose a task:\")\n",
        "    print(\"1. Sentiment Analysis\")\n",
        "    print(\"2. Text Summarization\")\n",
        "    print(\"3. English to Hindi Translation\")\n",
        "    print(\"0. Exit\")\n",
        "\n",
        "def sentiment_analysis():\n",
        "    text = input(\"\\nEnter text to analyze sentiment: \")\n",
        "    result = sentiment_pipeline(text)\n",
        "    print(f\"Sentiment: {result[0]['label']} (Confidence: {result[0]['score']:.2f})\")\n",
        "\n",
        "def summarization():\n",
        "    text = input(\"\\nEnter text to summarize: \")\n",
        "    summary = summarizer_pipeline(text, max_length=60, min_length=20, do_sample=False)\n",
        "    print(\"Summary:\", summary[0]['summary_text'])\n",
        "\n",
        "def translation():\n",
        "    text = input(\"\\nEnter English text to translate to Hindi: \")\n",
        "    translation = translator_pipeline(text)\n",
        "    print(\"Hindi Translation:\", translation[0]['translation_text'])\n",
        "\n",
        "def main():\n",
        "    while True:\n",
        "        show_menu()\n",
        "        choice = input(\"\\nEnter your choice (0-3): \")\n",
        "\n",
        "        if choice == '1':\n",
        "            sentiment_analysis()\n",
        "        elif choice == '2':\n",
        "            summarization()\n",
        "        elif choice == '3':\n",
        "            translation()\n",
        "        elif choice == '0':\n",
        "            print(\"\\nThank you for using AICLI Assistant. Goodbye! üëã\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid choice. Please select 0, 1, 2, or 3.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7T92WxRh355",
        "outputId": "a9a24fc2-65c2-45dc-9637-e2b995d5685b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Welcome to AICLI Assistant\n",
            "Please choose a task:\n",
            "1. Sentiment Analysis\n",
            "2. Text Summarization\n",
            "3. English to Hindi Translation\n",
            "0. Exit\n",
            "Invalid choice. Please select 0, 1, 2, or 3.\n",
            "\n",
            "ü§ñ Welcome to AICLI Assistant\n",
            "Please choose a task:\n",
            "1. Sentiment Analysis\n",
            "2. Text Summarization\n",
            "3. English to Hindi Translation\n",
            "0. Exit\n",
            "Sentiment: NEGATIVE (Confidence: 1.00)\n",
            "\n",
            "ü§ñ Welcome to AICLI Assistant\n",
            "Please choose a task:\n",
            "1. Sentiment Analysis\n",
            "2. Text Summarization\n",
            "3. English to Hindi Translation\n",
            "0. Exit\n",
            "Summary:  AICLI Assistant is a simple yet powerful command-line tool that uses Artificial Intelligence to assist users with common natural language tasks . Built using Python and Hugging Face Transformers, the assistant provides three core functionalities: Sentiment Analysis, Text Summarization, and English to Hindi Translation\n",
            "\n",
            "ü§ñ Welcome to AICLI Assistant\n",
            "Please choose a task:\n",
            "1. Sentiment Analysis\n",
            "2. Text Summarization\n",
            "3. English to Hindi Translation\n",
            "0. Exit\n",
            "Hindi Translation: ‡§ó‡•ç‡§≤‡•à‡§°‡§ø‡§Ø‡•á‡§ü‡§∞\n",
            "\n",
            "ü§ñ Welcome to AICLI Assistant\n",
            "Please choose a task:\n",
            "1. Sentiment Analysis\n",
            "2. Text Summarization\n",
            "3. English to Hindi Translation\n",
            "0. Exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Step 1: Load Pretrained Model and Tokenizer\n",
        "model_name = \"distilbert-base-cased-distilled-squad\"  # or any other QA model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Take user input\n",
        "context = input(\"üìÑ Enter context paragraph:\\n\")\n",
        "question = input(\"‚ùì Ask a question:\\n\")\n",
        "\n",
        "# Step 3: Tokenize inputs\n",
        "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
        "\n",
        "# Step 4: Get model predictions\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Step 5: Extract answer\n",
        "start = torch.argmax(outputs.start_logits)\n",
        "end = torch.argmax(outputs.end_logits) + 1\n",
        "\n",
        "answer_tokens = inputs['input_ids'][0][start:end]\n",
        "answer = tokenizer.decode(answer_tokens)\n",
        "\n",
        "# Step 6: Display result\n",
        "print(\"\\n‚úÖ Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "dQhADptukWz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A7S9DjVhoISk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}